from typing import List
from llm_integration.config import MAX_TOKENS, TEMPERATURE, LLM_API_KEY, DEFAULT_MODEL
from llm_integration.exceptions import LLMChainError
from groq import Groq


class LLMIntegrationWithLLaMA:
    """
    Integration with LLaMA 3 using Groq API.
    """

    def __init__(self):
        """
        Initialize the LLM integration with the Groq API and LLaMA model parameters.
        """
        try:
            self.client = Groq(api_key=LLM_API_KEY)  # Initialize the Groq client
            self.model = DEFAULT_MODEL
            # self.tokenizer = LlamaTokenizer.from_pretrained("decapoda-research/llama-7b-hf")
        except Exception as e:
            raise LLMChainError(f"Failed to initialize Groq client: {e}")

    # def count_tokens(self, text: str) -> int:
    #     """
    #     Count the number of tokens in a given text using LLaMA tokenizer.

    #     Args:
    #         text (str): The text to tokenize.

    #     Returns:
    #         int: The number of tokens.
    #     """
    #     try:
    #         return len(self.tokenizer.encode(text))
    #     except Exception as e:
    #         raise LLMChainError(f"Failed to count tokens: {e}")

    def generate_response(self, query: str, retrieved_docs: List[dict]) -> str:
        """
        Generate a response from the LLM using Groq API.

        Args:
            query (str): The user's query.
            retrieved_docs (List[dict]): List of retrieved documents.

        Returns:
            str: The response generated by the LLM.
        """
        try:
            # Format the context
            context = "\n".join([f"Document {i + 1}:\n{doc['text']}" for i, doc in enumerate(retrieved_docs)])

            # Construct the full input prompt
            prompt = (
                "You are a knowledgeable assistant. Use the following documents to answer the question.\n\n"
                f"Context:\n{context}\n\n"
                f"Question: {query}\n\n"
                "Answer:"
            )

            # Token count validation
            # token_count = self.count_tokens(prompt, model=self.model)
            # if token_count > MAX_TOKENS:
            #    raise LLMChainError(f"Prompt exceeds token limit ({token_count} > {MAX_TOKENS}).")

            # Generate response using the Groq API
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=TEMPERATURE,
                max_tokens=MAX_TOKENS,
                top_p=1.0,
                stream=False,
            )

            # Extract the assistant's reply
            response_content = completion.choices[0].message.content.strip()
            return response_content
        except Exception as e:
            raise LLMChainError(f"Failed to generate response with Groq LLaMA: {e}")
